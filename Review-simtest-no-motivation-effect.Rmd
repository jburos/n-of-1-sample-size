---
title: "Prototype simtest without a motivation effect"
author: "Jacqueline Buros"
date: "9/6/2019"
output: 
  github_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(kableExtra.latex.load_packages = F)
library(knitr)
library(kableExtra)
library(here)
library(tidyverse)
library(future)
library(brms)
source(here('simdata.function.R'))
future::plan(multicore)
ggplot2::theme_set(theme_minimal())

# set the seed & run parameters
seed <- 122355482
run_date <- '2019-09-06'
run_desc <- 'prototype_simtest_again'
run_formula <- study_completion ~ duration_5_days + duration_27_days + notify_moderate
set.seed(seed)

# specify the priors
response_prop <- 0.2

# load simulated data 
simd <- readRDS(here('.brms_fits',
                     glue::glue('{run_desc}_{run_date}_{seed}.simd.Rds')))
prior_fit <- attr(simd, 'prior_fit')

# load priors used to simulate data
priors <- readRDS(here('.brms_fits',
                              glue::glue('{run_desc}_{run_date}_{seed}.priors.Rds')))

# load fits using simulated data
simfits <- readRDS(here('.brms_fits',
                        glue::glue('{run_desc}_{run_date}_{seed}.fits.Rds')))
```

The purpose of this document is to review the quality of data simulations & fits to simulated data. Though we will be reviewing fits to simulated data at a particular sample size, this will serve as a prototype of the process used to evaluate the quality of results at various sample sizes. 

## Review simulation parameters

First we should review the parameters used to simulate our data. In this round of simulations, a fixed set of parameter values are used to simulate data for all of our draws. We have parameters for the completion rate according to duration consistent with 30%, 20%, and 10% response in the 5-day, 15-day and 27-day groups respectively where notification level is low. 

Increasing the notification level to "moderate" confers a modest increase in completion rate uniformly across these three groups. This is reflected by a fixed OR of 1.1 (roughly a 10% increase).

These are the "target" completion rates for each of our groups used to simulate our data.

```{r}
simd %>%
  dplyr::bind_rows(.id = '.draw') %>%
  dplyr::distinct(duration_group, notify_moderate, scales::percent(invlogit_linpred)) %>%
  kableExtra::kable(escape = FALSE)
```

Of course, each draw of simulated data has a different _observed_ completion rate. There are two sources of variation contributing to the observed completion rate in each draw. One is the composition of the particular study population -- both the notification rate and the assignment to duration groups is randomized and so one would expect _some_ imbalance in these assignments in practice. 

Looking at the group sizes over our draws can give a sense of how this would be expected to vary in practice for a sample of this size (640 participants).

```{r}
simd %>%
  dplyr::bind_rows(.id = '.draw') %>%
  dplyr::filter(as.integer(.draw) <= 5) %>%
  dplyr::group_by(.draw, duration_group, notify_moderate) %>%
  dplyr::summarise(group_n = n()) %>%
  dplyr::ungroup() %>%
  tidyr::spread(.draw, group_n) %>%
  kableExtra::kable()
```

This is very likely a modest influence on the observed rates, but it is nonetheless there.

The second source comes from the simulation of the outcome itself.

Here is a sample of the observed completion rates for each of our 6 covariate combinations.

```{r}
simd %>%
  dplyr::bind_rows(.id = '.draw') %>%
  dplyr::filter(as.integer(.draw) <= 10) %>%
  dplyr::mutate(.draw = factor(as.integer(.draw), ordered = T)) %>%
  dplyr::group_by(.draw, duration_group, study_duration, notify_moderate) %>% 
  dplyr::summarise(completion_rate = mean(study_completion),
                   linear_predictor = mean(invlogit_linpred)) %>% 
  ggplot(., aes(x = .draw, y = completion_rate, colour = duration_group)) + 
  geom_point(aes(shape = factor(notify_moderate), group = duration_group), position = position_dodge(width = 1)) + 
  ggtitle('Observed completion rate by covariate values',
          subtitle = 'for 10 draws of simulated data') +
  scale_shape_manual('Moderate notification level', values = c(6, 19))
```

## Review priors for the model fit

### Prior on `b_Intercept`

The `Intercept` term defines the proportion of the population in the 15-day study length meeting the criteria for a "sufficient" response.

I have put a fairly narrow prior centered at the 20% mark:

```{r prior-Intercept-plot, echo = FALSE}
tbl_df(list(sufficient_response = brms::inv_logit_scaled(
  brms::rstudent_t(n = 1000,
                   df = 10,
                   mu = brms::logit_scaled(response_prop),
                   sigma = 0.3)))) %>%
  ggplot(., aes(x = sufficient_response)) + 
  geom_density(fill = 'lightblue') +
  geom_vline(aes(xintercept = response_prop), linetype = 'dashed') + 
  ggtitle('Prior distribution on `b_Intercept` (proportion of responses meeting criteria)') + 
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  labs(caption = glue::glue('Vertical line shows value at x = {response_prop}'))
```

### Prior on `b_duration_X`

We originally started with what is a typical weakly informative prior (`normal(0, 1)`) on the `beta` effects describing the offset from the response at 15 days for the other two study durations of 5 & 27 days. However this led to much larger effect sizes than I would have expected (ie sufficient-response rates close to 80% in one group).

In this set of responses the prior on these betas is more narrow: `r priors %>% dplyr::filter(class == 'b' & coef == '') %>% dplyr::select(prior) %>% unlist()`.

Let's see how that translates into a more familiar metric such as an OR.


```{r prior-beta-duration-plot, echo = F}
tbl_df(list(`exp(beta)` = exp(
  rnorm(n = 1000, mean = 0, sd = 0.4)))) %>%
  ggplot(., aes(x = `exp(beta)`)) + 
  geom_density(fill = 'lightgrey') +
  geom_vline(aes(xintercept = exp(0)), linetype = 'dashed') + 
  ggtitle('Prior distribution on `b_duration_X` (duration impact on response)') + 
  labs(caption = glue::glue('Vertical line shows value at x = exp(0)'))
```

This suggests even this distribution is too large for our betas -- I say this because there is significant density at a two-fold difference in response depending on the study duration (however -- maybe this is right?).

### Prior on `b_notify_high`

Finally we will review the priors on our notification level.

Here we start with a more narrow prior than for the duration since I expect this will be a weaker effect.

Our prior is: `r priors %>% dplyr::filter(class == 'b' & coef == 'notify_high') %>% dplyr::select(prior) %>% unlist()`.

Let's see how that translates into a more familiar metric such as an OR.

```{r prior-beta-notify-plot, echo = F}
tbl_df(list(`exp(beta)` = exp(
  rnorm(n = 1000, mean = 0, sd = 0.1)))) %>%
  ggplot(., aes(x = `exp(beta)`)) + 
  geom_density(fill = 'lightpink') +
  geom_vline(aes(xintercept = exp(0)), linetype = 'dashed') + 
  ggtitle('Prior distribution on `b_notify_high` (notification impact on response)') + 
  labs(caption = glue::glue('Vertical line shows value at x = exp(0)'))
```

This is still centered at 0 (OR = 1) but with a more narrow distribution that we had used for the duration effect. Keep in mind that these are both starting points; we can improve upon them in future versions.

## Posterior fits

We can now review the posterior fits for each of these scenarios. First, let's do a "typical" or generic summary of the posterior vs the parameter values.

```{r post-fits}
# construct "true" parameter values used to simulate our data
sim_pars <- simd[[1]] %>% dplyr::select(starts_with('b_')) %>% dplyr::rename(b_Intercept = b_Intercept_15_days) %>% dplyr::distinct() %>% unlist()
# transform our "target" percentages into log(OR) as specified by the model
sim_pars <- c(sim_pars,
              b_duration_27_days = log((10/90) / (20/80)),
              b_duration_5_days = log((30/70) / (20/80)))
# limit to parameters in our model fit (pick first arbitrarily)
sim_pars <- sim_pars[names(sim_pars) %in% get_variables(simfits[[1]])]

# prototype review on a single fit
sim_fit <- simfits[[1]]
bayesplot::mcmc_recover_intervals(as.array(sim_fit, pars = names(sim_pars)), true = sim_pars) +
  geom_hline(aes(yintercept = 0), linetype = 'dashed')
```

Next, we will write a function to summarise the metrics we "really" (supposedly) care about in our particular use case.

I would say these are (for each parameter):

1. Width of posterior 90% CI (credible interval)
2. Does the true value fall within the 90% posterior credible interval (check calibration)
3. For regression parameters, estimate the Type S (sign) error rate [if CI excludes 0]
   - IE does this interval contain 0?
       - if so, consider this analysis as "inconclusive"
       - if *not*, call the beta as being < or > 0
   - compare this determination to the direction of the "true effect"
4. For regression parameters, estimate the Type M (magnitude) error rate [if CI excludes 0]
   - IE does this interval contain 0?
       - if so, consider this analysis as "inconclusive"
       - if *not*, use the median value of the parameter as the "estimated effect"
   - compare this determination to the value of the "true effect". Is it >2 times as large?

### Summary of model fits

Summarizing these metrics for our 100 fits at this sample size (n = 640):

```{r sim-results}
extract_and_summarise <- function(sim_fit, sim_pars, ci_widths = c(0.9, 0.95)) {
  sim_fit %>%
    tidybayes::spread_draws(`b_.*`, regex = TRUE) %>%
    tidyr::gather(parname, posterior_value, starts_with('b_')) %>%
    dplyr::group_by(parname) %>%
    tidybayes::median_qi(posterior_value, .width = c(ci_widths)) %>%
    dplyr::ungroup() %>%
    dplyr::inner_join(sim_pars %>% as.list() %>% tbl_df() %>% 
                      dplyr::distinct() %>% 
                      tidyr::gather(parname, true_value, starts_with('b'))
                      , by = "parname") %>%
    dplyr::group_by(parname, .width, true_value) %>%
    dplyr::summarise(ci_width = abs(.upper - .lower),
                     ci_contains_true_value = dplyr::between(true_value, left = .lower, right = .upper),
                     ci_contains_0 = dplyr::between(0, left = .lower, right = .upper),
                     ci_type_s_error = dplyr::case_when(ci_contains_0 ~ NA_integer_,
                                                        (posterior_value < 0) == (true_value < 0) ~ 0L,
                                                        (posterior_value < 0) != (true_value < 0) ~ 1L,
                                                        TRUE ~ NA_integer_),
                     ci_type_m_error = dplyr::case_when(ci_contains_0 ~ NA_integer_,
                                                        posterior_value / true_value > 2 ~ 1L,
                                                        true_value / posterior_value > 2 ~ 1L,
                                                        TRUE ~ 0L)
    ) %>%
    dplyr::ungroup()
}
res <- purrr::map_dfr(simfits,
                      extract_and_summarise,
                      sim_pars = sim_pars,
                      ci_widths = c(0.95),
                      .id = 'fit')

# now we are going to summarise the type_m & type_s error rate for these few fits
res %>% 
  dplyr::rename(width = .width) %>%
  dplyr::group_by(parname, width) %>%
  dplyr::summarise_at(.vars = vars(starts_with('ci_')),
                      .funs = list(~ mean(., na.rm = T))) %>%
  dplyr::mutate_at(.vars = vars(-parname, -width, -ci_width),
                   .funs = list(~ scales::percent(., accuracy = 0.1))) %>%
  kableExtra::kable(escape = FALSE)
```

The width of the credible intervals shown above, however, is on the log(OR) scale. It's not very easy to tell what that might look like in practice.

Let's review what the posterior density for the OR looks like in order to better appreciate this.

```{r}
simfits %>%
  purrr::map_dfr(~ tidybayes::spread_draws(., `b_.*`, regex = TRUE), .id = '.fit') %>%
  tidyr::gather(parname, value, starts_with('b_')) %>%
  dplyr::filter(parname %in% c('b_duration_27_days', 'b_duration_5_days', 'b_notify_moderate')) %>%
  ggplot(., aes(y = parname, x = exp(value), colour = parname, group = stringr::str_c(.fit, parname))) +
  ggridges::geom_density_ridges(alpha = 0, color = "#0000001A") +
  geom_vline(aes(xintercept = 1), linetype = 'dashed', colour = 'red') +
  scale_x_continuous('Odds Ratio') +
  scale_y_discrete('Parameter') +
  ggtitle('Posterior Densities of Regression Parameters for n = 640')
```







